<!DOCTYPE html>

<html>
  <head>
    <!-- <img src="images\veda\cvv.png" alt=""> -->
    <title>Vedavyas Chakkirala - Portfolio</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <link rel="stylesheet" href="assets/css/main.css" />
  </head>
  <body class="is-preload">
    <!-- Header -->
    <section id="header">
      <header>
        <span class="image avatar"
          ><img src="./images/veda/cvv.png" alt=""
        /></span>
        <h1 id="logo">
          <a
            href="https://drive.google.com/file/d/17UVwI51TO17Z6kK7AgbAjj6dIlHyDBVo/view"
            >Vedavyas Chakkirala (He/Him)</a
          >
        </h1>
        <p>Cloud Data Engineer</p>
      </header>
      <nav id="nav">
        <ul>
          <li><a href="#one" class="active">About Me</a></li>
          <li><a href="#two">Things I am good at</a></li>
          <li><a href="#three">My Education</a></li>
          <li><a href="#four">My Experience</a></li>
          <li><a href="#five">My Certifications</a></li>
          <li><a href="#six">My Projects</a></li>
          <li><a href="#seven">Leadership and Involvement</a></li>
          <li><a href="#eight">Contact</a></li>
        </ul>
      </nav>
      <footer>
        <ul class="icons">
          <li>
            <a
              href="https://www.linkedin.com/in/vedavyas-chakkirala/"
              class="icon brands fa-linkedin"
              ><span class="label">LinkedIn</span></a
            >
          </li>
          <li>
            <a
              href="https://github.com/Vedavyas17"
              class="icon brands fa-github"
              ><span class="label">Github</span></a
            >
          </li>
          <li>
            <a
              href="https://leetcode.com/Vedavyas_C/"
              class="icon brands cib-leetcode"
              ><span class="label">Leetcode</span></a
            >
          </li>
        </ul>
      </footer>
    </section>

    <!-- Wrapper -->
    <div id="wrapper">
      <!-- Main -->
      <div id="main">
        <!-- One -->
        <section id="one">
          <div class="image main" data-position="center">
            <img src="images/IUB_banner.jpeg" alt="" />
          </div>
          <div class="container">
            <header class="major">
              <h2>Hello Data Enthusiasts üë®‚Äçüíª!</h2>
              <p>I am Vedavyas. Thanks for dropping by my portfolio site!</p>
            </header>
            <p>
              As a passionate Data Engineer, I bring extensive experience in
              building scalable, efficient data pipelines and architectures that
              support complex data analysis and machine learning models. Skilled
              in a wide range of technologies including Python, SQL, Apache
              Spark, and Hadoop, I excel in designing and implementing robust
              solutions for big data challenges. My expertise extends to cloud
              platforms such as AWS and Azure, where I've deployed and managed
              data lakes and warehouses, optimizing for performance and
              cost-effectiveness.
            </p>
            <p>
              As I edge closer to my graduation in May 2025 from the prestigious
              Indiana University Bloomington, I am enthusiastically preparing to
              embark on my professional journey in the dynamic field of Cloud
              Data Engineering. The experiences I've gained thus far have honed
              a diverse skill set that I am eager to leverage in tackling
              real-world challenges within the ever-evolving tech landscape.
              Additionally, I am open to relocation. If you are in search of a
              dedicated and motivated Cloud/Data engineer, poised to learn and
              make a substantial impact, let's establish a connection!
            </p>
            <p>
              <strong>Technical Skills: </strong><br />
              Programming Languages: Python, SQL, R <br />
              Web Development: Django, Flask, React, HTML, CSS, Java Script
              <br />
              Database Tools: Databricks,Azure Data Factory, Jenkins, MySQL,
              Snowflake, MongoDB, PostgreSQL, Redshift, AWS Glue <br />
              Libraries/Frameworks: Apache Spark, Kafka, Airflow, Scikit-Learn,
              NumPy, Pandas, Matplotlib, Seaborn, Keras, TensorFlow <br />
              Machine Learning: Time-series, Regression, Classification, KNN,
              Decision Trees, SVM, Random forest, Naive Bayes, Bagging,
              Boosting, Clustering, NLP, AmazonSageMaker <br />
              Tools & Technologies : Tableau, PowerBI, Data Warehousing, Docker,
              Git, Celery
            </p>
          </div>
        </section>

        <!-- Two -->
        <section id="two">
          <div class="container">
            <h3>Things I am good at</h3>
            <br />
            <ul class="feature-icons">
              <li class="icon solid fa-book">
                <span style="font-family: sans-serif">Be open to Learn</span>
              </li>
              <li class="icon solid far fa-futbol">
                <span style="font-family: sans-serif"
                  >Play any outdoor sport</span
                >
              </li>
              <li class="icon solid fa-coffee">
                <span style="font-family: sans-serif"
                  >Engage in healthy discussions</span
                >
              </li>
              <li class="icon solid far fa-handshake">
                <span style="font-family: sans-serif"
                  >Be an excellent team player</span
                >
              </li>
              <li class="icon solid fa-bolt">
                <span style="font-family: sans-serif">Be a mentor</span>
              </li>
              <li class="icon solid far fa-hand-holding-heart">
                <span style="font-family: sans-serif"
                  >Volunteer in social activities</span
                >
              </li>
            </ul>
          </div>
        </section>

        <!-- Three -->
        <section id="three">
          <div class="container">
            <h3>My Education</h3>
            <h4>
              Master of Science in Data Science, Indiana University Bloomington,
              USA, <br />August 2023 - May 2025 (upcoming)
            </h4>
            <h5>CGPA: 3.6 out of 4</h5>
            <p>
              <strong>Coursework:</strong> Bigdata Applications, Advance
              Database Concepts, Applied Algorithms, Applied Database
              Techniques, Software Engineering, Computer Networks, Data Mining
            </p>
            <br />
            <h4>
              Bachelor of Technology in Computer Science, Anna University,
              Chennai, India, August 2017 - June 2021
            </h4>
            <h5>CGPA: 8.89 out of 10</h5>
            <p>
              <strong>Coursework:</strong> Data Structures, Database Management
              Systems, Computer Organization and Architecture, Operating
              Systems, Cloud Computing, Linux Programming
            </p>
          </div>
        </section>

        <!-- Four -->
        <section id="four">
          <div class="container">
            <h3>My Experience</h3>
            <h4>
              Techtinium, &nbsp; India, &nbsp; September 2022 - July 2023
              (Full-time)
            </h4>

            <strong>Data Engineer:</strong>
            <ul>
              <li>
                EArchitected a real-time data pipeline processing 1000+ orders per minute using AWS Kinesis, Spark Streaming (5-second micro-batches), and Amazon Redshift for efficient data ingestion and analytics.
              </li>
              <li>
                Implemented 10-minute watermarking in Spark, reducing late events by 99% and ensuring data timeliness. Improved accuracy of time-sensitive metrics like average delivery times and order completion rates.
              </li>
              <li>
                Orchestrated ETL workflows with Apache Airflow, automating dimension data loading and Spark job triggering. Achieved 40% reduction in data preparation time and enhanced data consistency across the pipeline.
              </li>
              <li>
                Implemented scalable Redshift data warehouse, integrating real-time fact data with dimension tables. Enabled complex SQL queries with sub-second response for up to 10 million daily orders.
              </li>
              <li>
                Created real-time QuickSight dashboards, providing insights within 30 seconds of data ingestion. Improved decision-making speed by 20% and increased customer satisfaction by 15%.
              </li>
              <li>
                Engineered Spark processing on EMR with schema enforcement, cleaning, and anomaly detection, ensuring high data quality for real-time analytics on order values and delivery performance.
              </li>
              <li>
                Implemented comprehensive security using AWS IAM, VPC endpoints, and security groups. Maintained data integrity and compliance while allowing necessary inter-service communication.
              </li>
              <li>
                Contributed to CI/CD pipeline with AWS CodeBuild, automating production deployments. Reduced deployment errors by 70% and enabled faster iteration on pipeline improvements.
              </li>
              <li>
                Worked with different product vendors on POC, Post Sales
                Implementation.
              </li>
            </ul>

            <h4>
              Kaar Technologies, &nbsp; India, &nbsp; July 2021 - August 2022
              (Full-time)
            </h4>
            <strong>Data Analyst:</strong>
            <ul>
              <li>
                Engineered a comprehensive end-to-end machine learning pipeline, seamlessly integrating data collection from AWS S3 and Snowflake, preprocessing, feature extraction, model development, evaluation, and deployment using Python, resulting in a robust and scalable ML infrastructure.
              </li>
              <li>
                Developed an computer vision solution utilizing convolutional neural networks for the Fashion Apparels dataset, achieving an impressive 87% accuracy in apparel categorization and significantly enhancing model performance for Ralph Lauren's e-commerce platform.
              </li>
              <li>
                Orchestrated sophisticated data transformation and manipulation workflows leveraging Python and SQL, ensuring flawless integration across all systems while boosting data accuracy by 30% throughout the entire analytics pipeline, leading to more reliable insights and decision-making.
              </li>
              <li>
                Spearheaded the development of an advanced customer segmentation ML model that outperformed vendor accuracy by 20%, driving more targeted marketing strategies and improved customer engagement.
              </li>
              <li>
                Streamlined ETL workflows by integrating Tableau dashboards with cloud data warehouses, facilitating data-driven decision-making processes for sales and marketing teams and enhancing overall operational efficiency.
              </li>
              <li>
                Successfully mentored 4 interns, fostering their professional growth and contributing to the development of future talent within the organization. Actively participated in the Campus Hiring Program, conducting technical interviews and playing a key role in identifying and recruiting top-tier candidates.
              </li>
              <li> 
                Led a proof of concept (POC) initiative exploring the capabilities of Dataiku, evaluating its potential for enhancing our data science and machine learning workflows. This effort provided valuable insights for future tool adoption and process improvements.
              </li>
              <li>
                Attained an impressive 89% evaluation during the internship tenure, culminating in a successful transition to a full-time role as a Data Analyst within the dynamic Data Analytics team.
              </li>
            </ul>
            <h4>
              Kaar Technologies, &nbsp; India, &nbsp; December 2020 - June 2021
              (Internship)
            </h4>
            <strong>Data Intern:</strong>
            <ul>
              <li>
                Engineered React-based Customer, Vendor, and Employee portals
                integrated via SAP PI/PO middleware, utilizing SAP as the
                backend. Devised automated chat-bot for user-friendly access,
                Employee Maintenance, and system health checks.
              </li>
              <li>
                Performed Exploratory Data Analysis and handled unbalanced
                datasets to train the ML models. Built an internal Chat-bot to
                create new incidents, automate data look-ups , etc., and
                integrated it with Flutter mobile application.
              </li>

              <li>
                Leveraged Microsoft Access and SQL to extract and manipulate raw
                data, generating ad-hoc reports that boosted data-driven
                decisions in Power BI, enhancing trend visualization and
                optimizing KPIs for a 22% increase in efficiency.
              </li>
              <li>
                Implemented thorough data quality checks for Access and Power
                BI, ensuring precise updates; resulted in smoother database
                operations and a 18% reduction in data discrepancies on
                dashboards.
              </li>
              <li>
                As the internship progressed, my commitment to excellence and
                the pursuit of knowledge became evident, leading to a seamless
                transition into a full-time role with the esteemed Data
                Analytics team.
              </li>
            </ul>
          </div>
        </section>

        <!-- Five -->
        <section id="five">
          <div class="container">
            <h3>My Certifications</h3>
            <p>
              I love doing certifications. Below are a few that I pursued so
              far. I am parallelly working on a couple more while you are
              reading this information.
            </p>
            <div class="features">
              <article>
                <!-- href="https://credentials.databricks.com/28ee168a-d15a-4a8e-8ee0-62ca67902db0#gs.2zix5r" -->
                <a
                  href="https://www.credly.com/badges/30fc1aa6-aa8e-4030-968a-8c2658d77ca8/"
                  class="image"
                  ><img
                    src="images\aws-certified-data-engineer-associate.png"
                    alt="AWS Certified Data Engineer ‚Äì Associate"
                /></a>
                <div class="inner">
                  <h4>AWS Certified Data Engineer ‚Äì Associate</h4>
                  <p>
                    Skills learned and applied: AWS Glue, Athena, Redshift,
                    DynamoDB, EC2, Amazon Kinesis and many other AWS Data
                    Engineering services
                  </p>
                </div>
              </article>
              <article>
                <!-- href="https://credentials.databricks.com/f39da7a0-3f72-43a7-ba07-898ad1759ccc#gs.4e0kdw" -->
                <a href="https://credentials.databricks.com/1184acaf-299d-416a-8596-fa1b2ce4aea3" class="image"
                  ><img
                    src="images/Databricks Data Engineer Associate Badge.png"
                    alt="Databricks Data Engineer Associate"
                /></a>
                <div class="inner">
                  <h4>Databricks certified Data Engineer Associate</h4>
                  <p>
                    Skills learned and applied: Apache Spark, Delta Lake,
                    Databricks, Lakehouse, Delta Live Tables, Data Pipelines,
                    ETL, Production, SQL, Python
                  </p>
                </div>
              </article>
              <article>
                <!-- href="https://credentials.databricks.com/28ee168a-d15a-4a8e-8ee0-62ca67902db0#gs.2zix5r" -->
                <a href="" class="image"
                  ><img
                    src="images/Databricks Spark Certified Badge.png"
                    alt="Databricks certified Spark 3.0 Developer Associate"
                /></a>
                <div class="inner">
                  <h4>Databricks Spark 3.0 Developer Associate</h4>
                  <p>
                    Skills learned and applied: Spark architecture, Spark SQL
                    functions, UDFs, DataFrames, Adaptive query execution,
                    Python
                  </p>
                </div>
              </article>
              <article>
                <!-- href="https://credentials.databricks.com/f39da7a0-3f72-43a7-ba07-898ad1759ccc#gs.4e0kdw" -->
                <a href="https://certificates.confluent.io/419bff36-9ee7-44b2-b168-1618ad393bac" class="image"
                  ><img
                    src="images\kafka.png"
                    alt="Apache Kafka developer"
                /></a>
                <div class="inner">
                  <h4>Apache Kafka Developer</h4>
                  <p>
                    Individuals who successfully complete the Confluent Fundamentals Accreditation have an understanding of Apache Kafka and Confluent Platform. Users are able to: explore use cases, have general knowledge of Kafka‚Äôs core concepts, understand the ability of Kafka as a highly scalable, highly available, and resilient real-time event streaming platform.
                  </p>
                </div>
              </article>
            
              <article>
                <!-- href="https://learn.microsoft.com/en-gb/users/venkataviswanathchittilla-8390/credentials/460e55c16de5a83" -->
                <a href="" class="image"
                  ><img
                    src="images/Microsoft-Power-Bi-Data-Analyst-Associate-logo.png"
                    alt="Micrsoft certified PowerBI Data Analyst"
                /></a>
                <div class="inner">
                  <h4>Microsoft certified PowerBI Data Analyst Associate</h4>
                  <p>
                    Skills learned and applied: Prepare the data, Model the
                    data, Visualize and analyze the data
                  </p>
                </div>
              </article>
              <article>
                <a href="#" class="image"
                  ><img
                    src="images/azure-fundamentals-600x600 (1).png"
                    alt="Azure Fundamentals and Azure Data Fundamentals"
                /></a>
                <div class="inner">
                  <h4>Azure Fundamentals and Azure Data Fundamentals</h4>
                  <p>
                    Skills learned and applied: Cloud concepts, Azure
                    architecture and services, Azure management and governance,
                    Describe core data concepts, Identify considerations for
                    relational data on Azure, Describe considerations for
                    working with non-relational data on Azure, Describe an
                    analytics workload on Azure
                  </p>
                </div>
              </article>
              <!-- <strong>Planning to do:</strong>
              <article>
                <a
                  href="https://learn.microsoft.com/en-us/users/venkataviswanathchittilla-8390/credentials/6ccfe88835232f25"
                  class="image"
                  ><img
                    src="images/azure-data-engineer-associate.png"
                    alt="Azure Data Engineer Associate"
                /></a>
                <div class="inner">
                  <h4>Azure Data Engineer Associate</h4>
                  <p>
                    Skills learned and applied: Design and implement data
                    storage, Develop data processing, Secure, monitor, and
                    optimize data storage and data processing
                  </p>
                </div>
              </article>
              <article>
                <a
                  href="https://www.credly.com/badges/c6a03552-229d-43be-872f-1374d2b81b57/public_url"
                  class="image"
                  ><img
                    src="images/mongodb-associate-developer.png"
                    alt="MongoDB Associate Developer"
                /></a>
                <div class="inner">
                  <h4>MongoDB Associate Developer</h4>
                  <p>
                    Skills learned and applied: CRUD, Data Modeling, Drivers,
                    MongoDB, MongoDB Document Model, MongoDB Indexes, MongoDB
                    Tools & Tooling, Python, Software Engineering
                  </p>
                </div>
              </article>

              <article>
                <a
                  href="https://www.credly.com/badges/66b38192-8da2-4371-8462-8ac3c6964f47/public_url"
                  class="image"
                  ><img
                    src="images/mongodb-associate-data-modeler.png"
                    alt="MongoDB Associate Data Modeler"
                /></a>
                <div class="inner">
                  <h4>MongoDB Certified Associate Data Modeler</h4>
                  <p>
                    Skills learned and applied: Data Governance, Data
                    Management, Data Modeling, Data Storage, JSON, MongoDB,
                    MongoDB Data, Query Languages
                  </p>
                </div>
              </article> -->
            </div>
          </div>
        </section>

        <!-- Six -->
        <section id="six">
          <div class="container">
            <h3>My Projects</h3>
            <p>Below are a few hands-on projects that I worked on.</p>
            <div class="features">
              <article></article>
                <a
                  href="https://github.com/Vedavyas17/Real-Time-Credit-Card-Fraud-Detection-Pipeline"
                  class="image"
                  ><img src="images\fraud detection.webp" alt="GrandFile share"
                /></a>
                <div class="inner">
                  <h4>
                    Real-Time-Credit-Card-Fraud-Detection-Pipeline
                  </h4>
                  <p>
                    This project focuses to avoid identity theft, which detects any unusual activity using credit card, which has skyrocketed in the current era. I've used AWS S3 bucket, AWS EMR, Hive, Hadoop, MongoDB, PySpark and Apache Kafka to do this project. I've attached all the necessary files to perform this project in the Media section. Following are the checks performed in my project to detect frauds in Credit Card Transactions:

The Transaction would be considered as fraudulent if the Transaction Amount exceeds the Upper Control Limit (UCL) considering the last 10 transactions of the card

The Transaction would be considered as fraudulent if the Credit Score is less than 200

The geo location of each transaction is captured and the distance and time is identified for them. Considering if the time taken between two transactions does not exceed the speed limit of 900 Km/hr based on the distance, the current transaction would be treated as genuine, if not, as fraudulent.

                 </p>
                  <p>
                    Skills learned and applied: AWS(S3, EMR), Hadoop, Hive, MongoDB, PySpark, Apache Kafka
                  </p>
                </div>
              </article>
              <article>
                <a
                  href="https://github.com/Vedavyas17/Spotify-E2E-DE-implementation-in-AWS"
                  class="image"
                  ><img src="images\veda\spotify.png" alt="GrandFile share"
                /></a>
                <div class="inner">
                  <h4>
                    Spotify Data Pipeline: Extract, Transform, and Analyze with
                    AWS
                  </h4>
                  <p>
                    This project involves building an end-to-end data pipeline
                    for Spotify data using AWS services. It encompasses
                    extracting data from the Spotify API, storing it in AWS S3,
                    and implementing automated transformation processes using
                    AWS Lambda. The pipeline includes scheduled data extraction,
                    data cleaning and formatting, and automated triggers for
                    transformation based on data updates. The transformed data
                    is then stored back in S3 with proper organization. The
                    project also leverages AWS Glue and Athena for creating
                    analytics tables and enabling efficient querying. Key skills
                    learned from this project include working with AWS services
                    (S3, Lambda, Glue, Athena), API integration, data extraction
                    and transformation, automated pipeline development,
                    cloud-based data storage and organization, and setting up
                    data analytics infrastructure. This comprehensive solution
                    provides a scalable and automated approach to processing and
                    analyzing Spotify data, offering valuable insights for
                    various analytical purposes.
                  </p>
                  <p>
                    Skills learned and applied: Application Design, Python,
                    Infrastructure As Code, Amazon Web Services, AWS Elastic
                    Cloud Compute, AWS Relational Database Service, AWS Simple
                    Storage Service
                  </p>
                </div>
              </article>
              <article>
                <a
                  href="https://github.com/Vedavyas17/Customer-Relation-Prediction"
                  class="image"
                  ><img
                    src="images/Project Employee Churn Prediction thumbnail.png"
                    alt="GrandFile share"
                /></a>
                <div class="inner">
                  <h4>Employee Churn Prediction</h4>
                  <p>
                    Retaining current employees is more difficult for the HR
                    team than recruiting new ones. Any business that loses one
                    of its valuable employees suffers a loss in terms of
                    productivity, time, money, and other factors. This loss
                    could be reduced if HR could able to foresee future
                    employees who were considering leaving their positions; as a
                    result, we looked into ways to address the employee turnover
                    issue from a machine learning perspective through this
                    project. When the time comes to lay off workers as part of
                    organizational changes, the corporation can use churn
                    modeling to make a rational decision rather than randomly
                    selecting layoff candidates.
                  </p>
                  <p>
                    Skills learned and applied: Data collection, Data Mining,
                    Data Pre Processing, Python, Pandas, Exploratory Data
                    Analysis, Dimensionality Reduction, Machine Learning
                    algorithms, Tuning ML model performance
                  </p>
                </div>
              </article>
              <article>
                <a
                  href="https://github.com/Vedavyas17/Tableau-Dashboard-for-HR-Data-along-with-EDA"
                  class="image"
                  ><img src="images\veda\hr_tableau.png" alt="GrandFile share"
                /></a>
                <div class="inner">
                  <h4>Tableau-Dashboard-for-HR-Data-along-with-EDA</h4>
                  <p>
                    I had the privilege of working on a transformative project
                    in which I oversaw the implementation of Tableau for
                    in-depth data analysis of our Human Resources data. Every
                    company has an HR Department that handles various
                    recruitment and placement tasks. In this project, I worked
                    with a massive dataset to extract valuable insights. These
                    insights can be really helpful for the HR department to
                    improve their work and gain a better understanding of the
                    recruitment process in the market. These dashboards
                    empowered with the insights needed for informed
                    decision-making, allowing to steer businesses in the right
                    direction and drive substantial improvements in overall
                    performance.
                  </p>
                  <p>
                    Skills learned and applied: Data Collection, Data Cleaning,
                    Data Transformation, Data Modeling, Data pre-processing,
                    Information Visualization, Business Intelligence, Tableau
                  </p>
                </div>
              </article>
            </div>
          </div>
        </section>

        <!-- seven -->
        <section id="seven">
          <div class="container">
            <h3>Leadership and Involvement</h3>
            <ul>
              <li>
                Played a pivotal role in mentoring new hires at Kaar
                Technologies by offering technical expertise and guidance,
                ensuring successful integration into the SAP Data Analytics
                team; resulted in a 30% decrease in onboarding time and
                increased overall team efficiency by 25%.
              </li>
              <li>
                Got BEST-PERFORMER-OF-THE-MONTH four times for best performance
                on the project.
              </li>
              <li>
                Participated in the Campus Hiring process and on-boarded many
                employees.
              </li>
              <li>
                Placed 5th among 21 teams at the on-site finals, Kaizen Robotics
                Program of IIT Madras, Chennai.
              </li>
            </ul>
          </div>
        </section>

        <!-- eight -->
        <section id="eight">
          <div class="container">
            <h3>Contact Me</h3>
            &#127968; Bloomington, Indiana, US.
            <p>
              If you have any potential opportunity for me, or just want to get
              in touch, please feel free to drop me an email. You can also reach
              out to me on LinkedIn from the bottom-right corner of this page.
              <br/ > <br/ >I look forward to connecting with other industry
              professionals, sharing knowledge, and exploring new opportunities
              in the data space. Because in this realm of information, a warm
              welcome is just a JOIN operation away. üòä
            </p>
            <form method="post" action="https://formspree.io/f/xpzvwdzw">
              <div class="row gtr-uniform">
                <div class="col-6 col-12-xsmall">
                  <input type="text" name="name" id="name" placeholder="Name" />
                </div>
                <div class="col-6 col-12-xsmall">
                  <input
                    type="email"
                    name="email"
                    id="email"
                    placeholder="Email"
                  />
                </div>
                <div class="col-12">
                  <input
                    type="text"
                    name="subject"
                    id="subject"
                    placeholder="Subject"
                  />
                </div>
                <div class="col-12">
                  <textarea
                    name="message"
                    id="message"
                    placeholder="Message"
                    rows="6"
                  ></textarea>
                </div>
                <div class="col-12">
                  <ul class="actions">
                    <li>
                      <input
                        type="submit"
                        class="primary"
                        value="Send Message"
                      />
                    </li>
                    <li><input type="reset" value="Reset Form" /></li>
                  </ul>
                </div>
              </div>
            </form>
          </div>
        </section>
      </div>

      <!-- Footer -->
      <section id="footer">
        <div class="container">
          <ul class="copyright">
            <li>&copy; Vedavyas Chakkirala. All rights reserved.</li>
          </ul>
        </div>
      </section>
    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
